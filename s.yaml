edition: 1.0.0
name: llm-client
access: "default"

vars:
  region: "cn-shanghai"
  service:
    name: "llm-client"
    description: 'llm client support for a variety of task'
  vpcConfig:
      vpcId: vpc-uf69o8x2c9dpe9ni9zhd6
      securityGroupId: sg-uf6dklm6ropymextgbrp
      vswitchIds:
        - vsw-uf6mjuikdtht9k3xq7rop

services:
  llm-client:
    component: fc
    actions: 
      pre-deploy: 
        - run: rm -rf deploy/*
          path: .
        - run: cp -r src/* deploy 
          path: .
    props: 
      region: ${vars.region}
      service: ${vars.service}
      function:
        name: "llm-client"
        description: 'llm client support for a variety of task'
        codeUri: './deploy'
        runtime: custom
        caPort: 8000
        timeout: 60
        layers:
          - acs:fc:cn-shanghai:official:layers/Python310/versions/2 # python3.10
          - acs:fc:cn-shanghai:1847940926016662:layers/python_llm/versions/4 # package like fastapi uvicorn
        environmentVariables:
          PATH: /opt/python3.10/bin:$PATH # python3.10
          PYTHONPATH: /opt/python # layer path
          LLM_HOST: https://example.com # proxy server for openai
          LLM_TOKENS: token1,token2 # to avoid rate limit
        customRuntimeConfig:
          command:
            - python
          args:
            - -u
            - main.py
      triggers:
        - name: httpTrigger
          type: http
          config:
            authType: anonymous
            methods:
              - GET
              - POST