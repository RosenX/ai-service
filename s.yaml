edition: 1.0.0
name: llm-client
access: "default"

vars:
  region: "us-west-1" # 硅谷
  service:
    name: "llm-client"
    description: 'llm client support for a variety of task'

services:
  llm-client:
    component: fc
    actions: 
      pre-deploy: 
        - run: rm -rf deploy/*
          path: .
        - run: cp -r src/* deploy 
          path: .
    props: 
      region: ${vars.region}
      service: ${vars.service}
      function:
        name: "llm-client"
        description: 'llm client support for a variety of task'
        codeUri: './deploy'
        runtime: custom
        caPort: 9000
        timeout: 60
        layers:
          - acs:fc:us-west-1:official:layers/Python310/versions/2 # python3.10
          - acs:fc:us-west-1:1847940926016662:layers/llm-client-python310-custom/versions/1 # python package
        environmentVariables:
          PATH: /opt/python3.10/bin:$PATH # python3.10
          PYTHONPATH: /opt/python # layer path
          LLM_HOST: https://example.com # proxy server for openai, default: https://api.openai.com/v1
          LLM_TOKENS: token
          LLM_TYPE: openai
          ENV: prod
        customRuntimeConfig:
          command:
            - python
          args:
            - -u
            - main.py
      triggers:
        - name: httpTrigger
          type: http
          config:
            authType: anonymous
            methods:
              - GET
              - POST